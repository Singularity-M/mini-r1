import gradio as gr
import torch
import warnings
from transformers import AutoModelForCausalLM, AutoTokenizer, StoppingCriteria, StoppingCriteriaList, TextIteratorStreamer
from model.model import MiniR1
from model.MiniR1Config import MiniR1Config
from model.model_lora import ModelWithLoRA
from threading import Thread
import base64
js_func = """
function refresh() {
    const url = new URL(window.location);

    if (url.searchParams.get('__theme') !== 'dark') {
        url.searchParams.set('__theme', 'dark');
        window.location.href = url.href;
    }
}
"""

css = """
html, body {
    height: 100% !important;
    margin: 0 !important;
    padding: 0 !important;
}
.gradio-container, .block-container {
    height: 100vh !important;
}
#chatbot {
    height: calc(100vh - 200px) !important;
}

"""

# è¯»å–å›¾ç‰‡å¹¶è½¬æ¢ä¸º Base64
with open("logo.png", "rb") as image_file:
    encoded_image = base64.b64encode(image_file.read()).decode("utf-8")

warnings.filterwarnings('ignore')


tokenizer = AutoTokenizer.from_pretrained('./model/minir1_tokenizer')
model_from = 1  # 1ä»æƒé‡ï¼Œ2ç”¨transformers
device = "cuda:0"
lm_config = MiniR1Config(ntk=8)


moe_path = '_moe' if lm_config.use_moe else ''
ckp = f'./out/sft_long_{lm_config.dim}{moe_path}.pth'

model = MiniR1(lm_config)
state_dict = torch.load(ckp, map_location=device)

# å¤„ç†ä¸éœ€è¦çš„å‰ç¼€
unwanted_prefix = '_orig_mod.'
for k, v in list(state_dict.items()):
    if k.startswith(unwanted_prefix):
        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)

    for k, v in list(state_dict.items()):
        if 'mask' in k:
            del state_dict[k]

        # åŠ è½½åˆ°æ¨¡å‹ä¸­
model.load_state_dict(state_dict, strict=False)
model = ModelWithLoRA(model)
model.load_lora_weights("./out/tcm_lora_512.pth")

model.eval()
model = model.to("cuda:0")

class StopOnTokens(StoppingCriteria):
    def __call__(self, input_ids: torch.LongTensor, scores: torch.FloatTensor, **kwargs) -> bool:
        stop_ids = [29, 0]
        for stop_id in stop_ids:
            if input_ids[0][-1] == stop_id:
                return True
        return False

def predict(message, history, system_prompt, temperature, top_k, repetition_penalty, max_tokens): 
    top_k = int(top_k)
    max_tokens = int(max_tokens)

    history_transformer_format = history + [[message, ""]]  # å°†èŠå¤©å†å²è½¬æ¢ä¸º Transformer æ¨¡å‹æ‰€éœ€çš„æ ¼å¼
    messages = []
    messages.append({"role": "user", "content": message})

        # print(messages)
    new_prompt = tokenizer.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True
        )[-(max_tokens - 1):]

    x = tokenizer(new_prompt).data['input_ids']
    x = (torch.tensor(x, dtype=torch.long, device=device)[None, ...])
    answers = ""

    with torch.no_grad():
        res_y = model.generate(x, tokenizer.eos_token_id, max_new_tokens=max_tokens, temperature=temperature,
                                   top_k=top_k, repetition_penalty=repetition_penalty, stream=True)

        try:
            y = next(res_y)
        except StopIteration:
            print("")
        history_idx = 0
        while y != None:
            answer = tokenizer.decode(y[0].tolist())
            if answer and answer[-1] == 'ï¿½':
                try:
                    y = next(res_y)
                except:
                    break
                continue
            if not len(answer):
                try:
                    y = next(res_y)
                except:
                    break
                continue
            yield answer[:]
            try:
                y = next(res_y)
            except:
                break
            history_idx = len(answer)
with gr.Blocks(title="Mini R1 power by singularity", theme=gr.themes.Soft(),  js=js_func, css=css) as demo:
    with gr.Sidebar(label="æ§åˆ¶é¢æ¿", elem_id="sidebar", open=False):
        description = gr.HTML(f"""
        <div style="
            border: 2px solid #00f3ff;
            border-radius: 10px;
            padding: 10px;
            margin: 10px 0;
            box-shadow: 0 0 15px #00f3ff;
        ">
        <img src="data:image/png;base64,{encoded_image}" 
                 alt="AI Core" 
                 style="width:140%; border-radius: 4px;"/>
        </div>
        """)
        
        gr.Markdown("""
        ## <i class="fa fa-cogs" style="color: #00f3ff;"></i> å‚æ•°é…ç½®
        """)  
        additional_inputs=[
            gr.Textbox("You are helpful AI.", 
                    label="ğŸ¤– ç³»ç»Ÿæç¤ºè¯­",
                    max_lines=3),
            gr.Slider(0, 1, 
                    label="ğŸŒ¡ï¸ æ¸©åº¦ç³»æ•°",
                    value=0.6,
                    info="0: ä¿å®ˆå›ç­”ï¼Œ1: å¹³è¡¡æ¨¡å¼"),
            gr.Slider(1, 100, step=1, 
                    label="ğŸ” Top-Ké‡‡æ ·",
                    value=16,
                    info="é€‰æ‹©å‰Kä¸ªæœ€å¯èƒ½çš„è¯"),
            gr.Slider(minimum=1, 
                    maximum=2, 
                    label="ğŸ”„ é‡å¤æƒ©ç½šç³»æ•°",
                    value=1.0,
                    info="æ§åˆ¶é‡å¤æƒ©ç½šåŠ›åº¦ï¼ˆ1: æ— æƒ©ç½šï¼Œ2: å¼ºæƒ©ç½šï¼‰"),
            gr.Slider(10, 4096, step=1,
                    label="ğŸ“ æœ€å¤§ç”Ÿæˆé•¿åº¦",
                    value=1024,
                    info="ç”Ÿæˆå†…å®¹çš„æœ€å¤§tokenæ•°"),
        ]
   
    webtitle=gr.Markdown("# Mini R1 Power by Singularity",)
    chat_interface = gr.ChatInterface(
            predict,
            additional_inputs=additional_inputs,
            chatbot=gr.Chatbot(elem_id="chatbot",value=[[None, "æ‚¨å¥½ï¼Œæˆ‘æ˜¯MINI R1 ç”±ä¸­å›½ä¸ªäººå¼€å‘è€…å¼€å‘çš„äººå·¥æ™ºèƒ½åº”ç”¨ï¼Œè¯·é—®ä½ æœ‰ä»€ä¹ˆé—®é¢˜å˜›ï¼Ÿ"]]),
            # ä»¥ä¸‹å¯é€‰é…ç½®å¯è°ƒæ•´è¾“å…¥åŒºåŸŸçš„é«˜åº¦
            textbox=gr.Textbox(container=False, stop_btn=True, submit_btn=True),
        )

def main():
    demo.launch(debug=True)


if __name__ == "__main__":
    main()
