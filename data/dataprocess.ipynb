{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\text{tr}(AB) = \\text{tr}(BA)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "text = r\"\\boxed{\\text{tr}(AB) = \\text{tr}(BA)}\"\n",
    "match = re.search(r\"\\\\boxed{(.*)}\", text)\n",
    "if match:\n",
    "    print(match.group(1))  # 输出: \\text{tr}(AB) = \\text{tr}(BA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可以校验的数据共计：36233行。不可校验的数据共计：69949行\n"
     ]
    }
   ],
   "source": [
    "#区分一下reason数据集中有多少可以验证的数据\n",
    "import json\n",
    "import regex as re\n",
    "import os\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/distill_r1_110k.jsonl\"\n",
    "output_file_path_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/distill_r1_validate_sft.jsonl\"\n",
    "output_file_path_2 = \"/home/mth/project_llm/mini_llm/data/origin_data/distill_r1_general_sft.jsonl\"\n",
    "\n",
    "def ensure_file_exists(file_path):\n",
    "    if not os.path.exists(file_path):\n",
    "        with open(file_path, 'w', encoding='utf-8') as f:\n",
    "            pass  # 创建空文件\n",
    "\n",
    "pattern = r'\\\\boxed\\{.*?\\}'\n",
    "validate_pattern = r\"\\\\boxed{(.*)}\"\n",
    "\n",
    "output_datas_1 = []\n",
    "output_datas_2 = []\n",
    "\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip()) \n",
    "        if re.search(pattern, data['content']):\n",
    "            match = re.search(r\"\\\\boxed{(.*)}\", data['content'])\n",
    "            validate = match.group(1)\n",
    "            if data['score'] == 10:\n",
    "                new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"{data['input']}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"<think>\\n{data['reasoning_content']}\\n</think>\\n{data['content']}\"}\n",
    "                    ],\n",
    "                    \"validate\":f\"{validate}\"\n",
    "                }\n",
    "                output_datas_1.append(new_data)\n",
    "        else:\n",
    "            if data['score'] >=6:\n",
    "                new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"{data['input']}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"<think>\\n{data['reasoning_content']}\\n</think>\\n{data['content']}\"}\n",
    "                    ],\n",
    "                    \"validate\":\"\"\n",
    "                }\n",
    "                output_datas_2.append(new_data)\n",
    "\n",
    "print(f\"可以校验的数据共计：{len(output_datas_1)}行。不可校验的数据共计：{len(output_datas_2)}行\")\n",
    "with open(output_file_path_1, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_datas_1, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "with open(output_file_path_2, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_datas_2, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共计医学问答数据：112565行\n"
     ]
    }
   ],
   "source": [
    "# 处理医疗问答数据集\n",
    "\n",
    "import json\n",
    "import regex as re\n",
    "import os\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/ChatMed_TCM-v0.2.json\"\n",
    "output_file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/ChatMed_TCM_sft.jsonl\"\n",
    "\n",
    "output_datas = []\n",
    "\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip()) \n",
    "        query = data['query'].split(\"要求：\")[0]\n",
    "    \n",
    "            \n",
    "        new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"{query}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"{data['response']}\"}\n",
    "                    ]\n",
    "                }\n",
    "        output_datas.append(new_data)\n",
    "\n",
    "print(f\"共计医学问答数据：{len(output_datas)}行\")\n",
    "with open(output_file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_datas, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "过滤后的syndrome数量:\n",
      "   syndrome  count\n",
      "1     寒凝血瘀证    102\n",
      "6     心血不足证    109\n",
      "7     心血瘀阻证    441\n",
      "9     正虚瘀结证   1907\n",
      "13    气滞血瘀证   4720\n",
      "16    气虚血瘀证   7783\n",
      "19    气血亏虚证    262\n",
      "21    气阴两虚证    824\n",
      "23    湿热下注证   4394\n",
      "24    湿热内蕴证    116\n",
      "25    湿热瘀阻证    627\n",
      "26    湿热蕴结证    507\n",
      "27    湿热阻络证    120\n",
      "28    热毒壅结证    868\n",
      "31    痰浊瘀阻证    304\n",
      "32    痰浊蒙窍证    233\n",
      "33    痰湿中阻证    188\n",
      "34  痰湿蒙闭心窍证    175\n",
      "35    痰湿蕴肺证   1797\n",
      "38    痰瘀互结证    293\n",
      "39    痰瘀痹阻证    145\n",
      "42    瘀血阻络证    270\n",
      "45    肝经湿热证    170\n",
      "46    肝肾不足证    515\n",
      "47    肝肾亏虚证    907\n",
      "48    肝肾阴虚证    196\n",
      "50    肝胃不和证    540\n",
      "51    肝胃郁热证   1897\n",
      "56    肝阳上亢证    206\n",
      "62      肾虚证    584\n",
      "63    脾肾两虚证    894\n",
      "65    脾胃不和证    146\n",
      "68     脾胃虚证    169\n",
      "69    脾胃阳虚证    540\n",
      "73      血热证    162\n",
      "74    血瘀痰滞证    180\n",
      "76    阳虚水泛证    495\n",
      "80    阴虚火旺证    177\n",
      "82    阴虚血瘀证    130\n",
      "83    阴虚阳亢证    305\n",
      "84    风寒外袭证    400\n",
      "85   风寒湿痹阻证    122\n",
      "87    风寒袭肺证    507\n",
      "92    风痰上攻证    402\n",
      "93    风痰入络证    318\n",
      "94    风痰阻络证   3148\n",
      "过滤后的合并数据已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_new.csv\n",
      "过滤后的syndrome病例数量统计已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/syndrome_count_filtered_new.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 文件路径\n",
    "file_name_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/test_set.csv\"\n",
    "file_name_2 = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation.csv\"\n",
    "output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_new.csv\"\n",
    "output_count_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/syndrome_count_filtered_new.csv\"\n",
    "\n",
    "# 读取两个文件\n",
    "df1 = pd.read_csv(file_name_2)\n",
    "# df2 = pd.read_csv(file_name_2)\n",
    "\n",
    "# 合并数据\n",
    "# merged_df = pd.concat([df1, df2], ignore_index=True)\n",
    "merged_df = df1\n",
    "# 统计每个syndrome的病例数量\n",
    "syndrome_count = merged_df.groupby('syndrome').size().reset_index(name='count')\n",
    "\n",
    "# 过滤掉病例数不足30个的syndrome\n",
    "filtered_syndrome_count = syndrome_count[syndrome_count['count'] >= 100]\n",
    "\n",
    "# 打印过滤后的syndrome数量\n",
    "print(f\"过滤后的syndrome数量:\\n{filtered_syndrome_count}\")\n",
    "\n",
    "# 根据过滤后的syndrome列表，保留满足条件的数据\n",
    "filtered_merged_df = merged_df[merged_df['syndrome'].isin(filtered_syndrome_count['syndrome'])]\n",
    "\n",
    "# 保存过滤后的合并数据\n",
    "filtered_merged_df.to_csv(output_path, index=False)\n",
    "print(f\"过滤后的合并数据已保存到: {output_path}\")\n",
    "\n",
    "# 保存过滤后的syndrome病例数量统计\n",
    "filtered_syndrome_count.to_csv(output_count_path, index=False)\n",
    "print(f\"过滤后的syndrome病例数量统计已保存到: {output_count_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_train_set.csv\n",
      "测试集已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_test_set.csv\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation.csv\"\n",
    "train_output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_train_set.csv\"\n",
    "test_output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_test_set.csv\"\n",
    "\n",
    "filtered_merged_df = pd.read_csv(output_path)\n",
    "\n",
    "# 将每个syndrome随机抽取6个作为训练集，剩下的作为测试集\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for syndrome in filtered_merged_df['syndrome'].unique():\n",
    "    # 提取当前syndrome的数据\n",
    "    syndrome_data = filtered_merged_df[filtered_merged_df['syndrome'] == syndrome]\n",
    "    \n",
    "    # 随机抽取6个作为训练集，剩余的作为测试集\n",
    "    train_data, test_data = train_test_split(syndrome_data, train_size=6, random_state=42)\n",
    "    \n",
    "    # 添加到训练集和测试集列表\n",
    "    train_list.append(train_data)\n",
    "    test_list.append(test_data)\n",
    "\n",
    "# 合并训练集和测试集\n",
    "train_set = pd.concat(train_list, ignore_index=True)\n",
    "test_set = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "# 保存训练集和测试集\n",
    "train_set.to_csv(train_output_path, index=False)\n",
    "test_set.to_csv(test_output_path, index=False)\n",
    "print(f\"训练集已保存到: {train_output_path}\")\n",
    "print(f\"测试集已保存到: {test_output_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "训练集已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_train.csv\n",
      "验证集已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_val.csv\n",
      "测试集已保存到: /home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_test.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 文件路径\n",
    "output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation.csv\"\n",
    "train_output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_train.csv\"\n",
    "val_output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_val.csv\"\n",
    "test_output_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_test.csv\"\n",
    "\n",
    "# 读取数据\n",
    "filtered_merged_df = pd.read_csv(output_path)\n",
    "\n",
    "# 抽取比例\n",
    "train_ratio = 0.7\n",
    "val_ratio = 0.1\n",
    "test_ratio = 0.3\n",
    "\n",
    "# 存放每个类别的训练/验证/测试数据\n",
    "train_list = []\n",
    "val_list = []\n",
    "test_list = []\n",
    "\n",
    "# 每个syndrome单独划分\n",
    "for syndrome in filtered_merged_df['syndrome'].unique():\n",
    "    syndrome_data = filtered_merged_df[filtered_merged_df['syndrome'] == syndrome]\n",
    "    \n",
    "    # 先按训练+剩余\n",
    "    train_data, temp_data = train_test_split(syndrome_data, train_size=train_ratio, random_state=42)\n",
    "\n",
    "    # 再把剩余的分成验证集和测试集\n",
    "    val_data, test_data = train_test_split(temp_data, train_size=val_ratio / (val_ratio + test_ratio), random_state=42)\n",
    "\n",
    "    # 加到列表\n",
    "    train_list.append(train_data)\n",
    "    val_list.append(val_data)\n",
    "    test_list.append(test_data)\n",
    "\n",
    "# 合并\n",
    "train_set = pd.concat(train_list, ignore_index=True)\n",
    "val_set = pd.concat(val_list, ignore_index=True)\n",
    "test_set = pd.concat(test_list, ignore_index=True)\n",
    "\n",
    "# 保存\n",
    "train_set.to_csv(train_output_path, index=False)\n",
    "val_set.to_csv(val_output_path, index=False)\n",
    "test_set.to_csv(test_output_path, index=False)\n",
    "\n",
    "print(f\"训练集已保存到: {train_output_path}\")\n",
    "print(f\"验证集已保存到: {val_output_path}\")\n",
    "print(f\"测试集已保存到: {test_output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共计数据：23753行\n"
     ]
    }
   ],
   "source": [
    "#处理中医书籍数据集\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/PT_University_TCM_books.csv\"\n",
    "output_file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/TCM_book_sft.jsonl\"\n",
    "\n",
    "output_datas = []\n",
    "\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    \n",
    "        # 如果有标题行，可以跳过标题行\n",
    "    next(csvreader)  # 跳过标题行\n",
    "\n",
    "        # 遍历每一行\n",
    "    for line in csvreader:\n",
    "        new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"{line[0]}\"}\n",
    "                    ]\n",
    "                }\n",
    "        output_datas.append(new_data)\n",
    "\n",
    "print(f\"共计数据：{len(output_datas)}行\")\n",
    "with open(output_file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_datas, file, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共计医学问答数据：792099行\n"
     ]
    }
   ],
   "source": [
    "#处理中医问答数据集\n",
    "\n",
    "import json\n",
    "import csv\n",
    "import ast\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/PT_dialogue.csv\"\n",
    "output_file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/TCM_dialogue_sft.jsonl\"\n",
    "\n",
    "output_datas = []\n",
    "\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    csvreader = csv.reader(file)\n",
    "    \n",
    "        # 如果有标题行，可以跳过标题行\n",
    "    next(csvreader)  # 跳过标题行\n",
    "\n",
    "        # 遍历每一行\n",
    "    for line in csvreader:\n",
    "        line = ast.literal_eval(line[0])\n",
    "        pass\n",
    "        new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"{line[2]}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"{line[3]}\"}\n",
    "                    ]\n",
    "                }\n",
    "        output_datas.append(new_data)\n",
    "\n",
    "print(f\"共计医学问答数据：{len(output_datas)}行\")\n",
    "with open(output_file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_datas, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共计医学问答数据：280280行\n"
     ]
    }
   ],
   "source": [
    "#处理lora数据集\n",
    "import json\n",
    "import regex as re\n",
    "import random\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/Medical_consultation_data_SFT/med_6part/med_6.json\"\n",
    "file_path_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/Medical_consultation_data_SFT/med_zh/train_zh.json\"\n",
    "output_file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/ChatMed_lora.jsonl\"\n",
    "\n",
    "output_datas = []\n",
    "\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip()) \n",
    "    \n",
    "            \n",
    "        new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"{data['instruction']}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"{data['output']}\"}\n",
    "                    ]\n",
    "                }\n",
    "        output_datas.append(new_data)\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip()) \n",
    "    \n",
    "            \n",
    "        new_data = {\n",
    "                    \"conversations\": [\n",
    "                        {\"role\": \"user\", \"content\": f\"{data['instruction']}\"},\n",
    "                        {\"role\": \"assistant\", \"content\": f\"{data['output']}\"}\n",
    "                    ]\n",
    "                }\n",
    "        output_datas.append(new_data)\n",
    "\n",
    "print(f\"共计医学问答数据：{len(output_datas)}行\")\n",
    "with open(output_file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    random.shuffle(output_datas)\n",
    "    for data in output_datas:\n",
    "        file.write(json.dumps(data, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共计问答数据：11381621行\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 33\u001b[0m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m共计问答数据：\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(output_datas)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m行\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(output_file_path, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mw\u001b[39m\u001b[38;5;124m\"\u001b[39m, encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m file:\n\u001b[0;32m---> 33\u001b[0m     \u001b[43mjson\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_datas\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mensure_ascii\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindent\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/json/__init__.py:179\u001b[0m, in \u001b[0;36mdump\u001b[0;34m(obj, fp, skipkeys, ensure_ascii, check_circular, allow_nan, cls, indent, separators, default, sort_keys, **kw)\u001b[0m\n\u001b[1;32m    173\u001b[0m     iterable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(skipkeys\u001b[38;5;241m=\u001b[39mskipkeys, ensure_ascii\u001b[38;5;241m=\u001b[39mensure_ascii,\n\u001b[1;32m    174\u001b[0m         check_circular\u001b[38;5;241m=\u001b[39mcheck_circular, allow_nan\u001b[38;5;241m=\u001b[39mallow_nan, indent\u001b[38;5;241m=\u001b[39mindent,\n\u001b[1;32m    175\u001b[0m         separators\u001b[38;5;241m=\u001b[39mseparators,\n\u001b[1;32m    176\u001b[0m         default\u001b[38;5;241m=\u001b[39mdefault, sort_keys\u001b[38;5;241m=\u001b[39msort_keys, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkw)\u001b[38;5;241m.\u001b[39miterencode(obj)\n\u001b[1;32m    177\u001b[0m \u001b[38;5;66;03m# could accelerate with writelines in some versions of Python, at\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;66;03m# a debuggability cost\u001b[39;00m\n\u001b[0;32m--> 179\u001b[0m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43miterable\u001b[49m\u001b[43m:\u001b[49m\n\u001b[1;32m    180\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mchunk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/json/encoder.py:430\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode\u001b[0;34m(o, _current_indent_level)\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m _floatstr(o)\n\u001b[1;32m    429\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, (\u001b[38;5;28mlist\u001b[39m, \u001b[38;5;28mtuple\u001b[39m)):\n\u001b[0;32m--> 430\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_list(o, _current_indent_level)\n\u001b[1;32m    431\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(o, \u001b[38;5;28mdict\u001b[39m):\n\u001b[1;32m    432\u001b[0m     \u001b[38;5;28;01myield from\u001b[39;00m _iterencode_dict(o, _current_indent_level)\n",
      "File \u001b[0;32m/opt/miniconda3/lib/python3.12/json/encoder.py:326\u001b[0m, in \u001b[0;36m_make_iterencode.<locals>._iterencode_list\u001b[0;34m(lst, _current_indent_level)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    325\u001b[0m             chunks \u001b[38;5;241m=\u001b[39m _iterencode(value, _current_indent_level)\n\u001b[0;32m--> 326\u001b[0m         \u001b[38;5;28;01myield from\u001b[39;00m chunks\n\u001b[1;32m    327\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m newline_indent \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    328\u001b[0m     _current_indent_level \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_trace_dispatch_regular.py:436\u001b[0m, in \u001b[0;36mThreadTracer.__call__\u001b[0;34m(self, frame, event, arg)\u001b[0m\n\u001b[1;32m    430\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m event \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcall\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m NO_FTRACE\n\u001b[1;32m    432\u001b[0m \u001b[38;5;66;03m# if DEBUG: print('trace_dispatch', filename, frame.f_lineno, event, frame.f_code.co_name, file_type)\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \n\u001b[1;32m    434\u001b[0m \u001b[38;5;66;03m# Just create PyDBFrame directly (removed support for Python versions < 2.5, which required keeping a weak\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[38;5;66;03m# reference to the frame).\u001b[39;00m\n\u001b[0;32m--> 436\u001b[0m ret \u001b[38;5;241m=\u001b[39m \u001b[43mPyDBFrame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    437\u001b[0m \u001b[43m    \u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    438\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpy_db\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mabs_path_canonical_path_and_base\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madditional_info\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_skips_cache\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mframe_cache_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    439\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    440\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mtrace_dispatch(frame, event, arg)\n\u001b[1;32m    441\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m ret \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    442\u001b[0m     \u001b[38;5;66;03m# 1 means skipped because of filters.\u001b[39;00m\n\u001b[1;32m    443\u001b[0m     \u001b[38;5;66;03m# 2 means skipped because no breakpoints were hit.\u001b[39;00m\n\u001b[1;32m    444\u001b[0m     cache_skips[frame_cache_key] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m2\u001b[39m\n",
      "File \u001b[0;32m~/.local/lib/python3.12/site-packages/debugpy/_vendored/pydevd/_pydevd_bundle/pydevd_frame.py:155\u001b[0m, in \u001b[0;36mPyDBFrame.__init__\u001b[0;34m(self, args)\u001b[0m\n\u001b[1;32m    152\u001b[0m should_skip \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m  \u001b[38;5;66;03m# Default value in class (put in instance on set).\u001b[39;00m\n\u001b[1;32m    153\u001b[0m exc_info \u001b[38;5;241m=\u001b[39m ()  \u001b[38;5;66;03m# Default value in class (put in instance on set).\u001b[39;00m\n\u001b[0;32m--> 155\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, args):\n\u001b[1;32m    156\u001b[0m     \u001b[38;5;66;03m# args = main_debugger, abs_path_canonical_path_and_base, base, info, t, frame\u001b[39;00m\n\u001b[1;32m    157\u001b[0m     \u001b[38;5;66;03m# yeap, much faster than putting in self and then getting it from self later on\u001b[39;00m\n\u001b[1;32m    158\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_args \u001b[38;5;241m=\u001b[39m args\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# ENDIF\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# 处理工匠数据集\n",
    "\n",
    "import json\n",
    "import regex as re\n",
    "import os\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/sft_data_zh.jsonl\"\n",
    "output_file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/gongjiang_data_sft.jsonl\"\n",
    "\n",
    "output_datas = []\n",
    "\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip()) \n",
    "        conversations = [\n",
    "            {\"role\": \"user\", \"content\": f\"{data['input']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"{data['output']}\"}\n",
    "            ]\n",
    "\n",
    "        if data['history']:\n",
    "            for conversation in data['history']:\n",
    "                conversations.append({\"role\": \"user\", \"content\": f\"{conversation[0]}\"})\n",
    "                conversations.append({\"role\": \"assistant\", \"content\": f\"{conversation[1]}\"})\n",
    "        pass\n",
    "        new_data = {\n",
    "                    \"conversations\": conversations\n",
    "                }\n",
    "        output_datas.append(new_data)\n",
    "\n",
    "print(f\"共计问答数据：{len(output_datas)}行\")\n",
    "with open(output_file_path, mode=\"w\", encoding=\"utf-8\") as file:\n",
    "    json.dump(output_datas, file, ensure_ascii=False, indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "成功加载 10 条数据, 共计 4109472 条数据\n",
      "{'tokens': [1, 62, 1313, 7177, 1395, 1091, 6840, 1593, 266, 2377, 3088, 5541, 882, 2251, 881, 547, 3138, 3607, 5176, 268, 331, 1193, 882, 2251, 1538, 1091, 881, 547, 882, 2251, 2515, 523, 2466, 1827, 266, 1419, 390, 1197, 268, 304, 1091, 881, 547, 882, 2251, 2237, 264, 3138, 5176, 727, 7173, 3088, 474, 4, 3921, 264, 2734, 2579, 3614, 2319, 268, 2344, 291, 644, 62, 1313, 7177, 1395, 1091, 6840, 1593, 592, 266, 2377, 3088, 5541, 882, 2251, 881, 547, 3607, 5176, 264, 1242, 331, 1193, 882, 2251, 266, 1538, 6183, 389, 881, 547, 510, 523, 2466, 1827, 266, 1419, 390, 1197, 268, 1532, 693, 9070, 881, 547, 331, 1193, 882, 2251, 2237, 264, 5176, 727, 7173, 2329, 268, 1280, 264, 308, 515, 405, 4787, 2344, 755, 3437, 5467, 4926, 268, 2344, 727, 3528, 1524, 881, 547, 882, 2251, 503, 2515, 2466, 1827, 264, 4896, 880, 3812, 2466, 1827, 4684, 2377, 3088, 264, 368, 2415, 474, 1242, 331, 2377, 3088, 291, 4656, 523, 2466, 1827, 266, 1419, 390, 1197, 268, 4896, 644, 264, 881, 547, 2515, 729, 5712, 266, 1419, 390, 1197, 3759, 2377, 3088, 268, 1325, 2472, 264, 8270, 4362, 610, 363, 1787, 364, 464, 2319, 268, 3142, 62, 1313, 7177, 304, 881, 547, 882, 2251, 2237, 266, 5176, 2466, 1827, 341, 93, 3088, 268, 1365, 264, 609, 1091, 881, 547, 2377, 3088, 510, 264, 1221, 2466, 1827, 1651, 93, 1559, 7737, 268, 8755, 264, 331, 1193, 882, 2251, 266, 1538, 662, 4896, 2377, 3088, 643, 3759, 523, 2466, 1827, 266, 1419, 390, 1197, 268, 746, 1232, 5969, 380, 541, 297, 2377, 3088, 3759, 1419, 390, 1197, 4499, 2466, 1827, 268, 4896, 297, 2377, 642, 1641, 22, 20, 25, 14, 13, 93, 1559, 7737, 14, 2993, 3499, 464, 636, 4298, 268, 1280, 264, 763, 1574, 599, 7077, 25, 264, 5290, 1018, 751, 6159, 268, 1325, 264, 2061, 1574, 4171, 2377, 15, 25, 264, 2096, 1574, 1651, 93, 1559, 7737, 268, 856, 1290, 2061, 1574, 264, 2377, 2289, 25, 3759, 2914, 264, 1370, 297, 2914, 642, 2843, 1559, 7737, 1242, 264, 464, 636, 4298, 2472, 264, 694, 1395, 763, 1574, 599, 836, 2377, 264, 1466, 93, 5152, 268, 2914, 836, 2377, 3759, 7053, 264, 1370, 93, 3759, 7053, 268, 746, 4896, 644, 264, 62, 1313, 7177, 304, 881, 547, 882, 2251, 2237, 266, 5176, 2466, 1827, 291, 7053, 3088, 474, 2591, 264, 308, 515, 2982, 1290, 636, 1698, 9750, 268, 3142, 880, 3812, 2466, 1827, 291, 7053, 3088, 264, 881, 547, 2377, 510, 4171, 2914, 3088, 268, 9173, 264, 2377, 3088, 5850, 636, 2914, 3088, 266, 1419, 390, 1197, 1460, 474, 8187, 264, 1007, 2914, 266, 1419, 390, 9196, 2377, 264, 2246, 402, 2504, 448, 516, 268, 1370, 636, 4810, 368, 266, 474, 1132, 3467, 549, 308, 4540, 9344, 933, 328, 474, 6704, 264, 2344, 727, 644, 1524, 608, 882, 2251, 1538, 1091, 881, 547, 2515, 523, 2466, 1827, 266, 1419, 390, 1197, 2129, 9120, 882, 2251, 1538, 291, 2377, 3088, 264, 3455, 3759, 881, 547, 2515, 1419, 390, 1197, 264, 1370, 7491, 268, 1108, 645, 645, 3467, 917, 4772, 268, 1279]}\n",
      "{'tokens': [3455, 3759, 881, 547, 2515, 1419, 390, 1197, 264, 1370, 7491, 268, 1108, 645, 645, 3467, 917, 4772, 268, 1279, 264, 687, 880, 3812, 2466, 1827, 1471, 7053, 264, 746, 8565, 309, 917, 464, 474, 6704, 264, 3142, 880, 3812, 2466, 1827, 291, 917, 5467, 268, 1279, 264, 3142, 880, 2466, 1827, 291, 94, 264, 1365, 881, 547, 2515, 2466, 1827, 291, 94, 16, 2377, 268, 744, 448, 516, 264, 2377, 3759, 13, 94, 16, 2377, 14, 20, 25, 268, 464, 636, 4298, 2472, 264, 1466, 94, 16, 2377, 34, 2914, 264, 1370, 94, 34, 7053, 268, 331, 569, 696, 3120, 1209, 264, 8187, 268, 1370, 3703, 2761, 268, 1370, 1960, 7594, 3703, 7053, 3088, 264, 368, 1052, 474, 746, 881, 547, 882, 2251, 637, 3138, 5176, 727, 694, 309, 7053, 3088, 268, 9173, 881, 547, 2377, 264, 4171, 2914, 264, 520, 2377, 2246, 402, 291, 2914, 266, 1419, 390, 1197, 264, 1325, 694, 2504, 2344, 266, 2113, 328, 268, 5, 62, 1313, 7177, 881, 547, 882, 2251, 9972, 2466, 1827, 617, 341, 65, 13, 2843, 4768, 14, 3088, 268, 881, 547, 2377, 3088, 510, 264, 523, 2466, 1827, 341, 65, 13, 2843, 1559, 7737, 4768, 7372, 744, 448, 516, 264, 2377, 3088, 291, 523, 2466, 1827, 266, 65, 13, 65, 9536, 1233, 96, 22, 98, 96, 25, 98, 65, 6315, 1212, 297, 65, 64, 2377, 642, 4768, 9536, 1233, 96, 22, 98, 96, 25, 98, 13, 93, 1559, 7737, 14, 65, 66, 463, 380, 541, 763, 1574, 7077, 25, 297, 65, 64, 2914, 642, 2843, 1559, 7737, 65, 66, 464, 515, 297, 65, 64, 93, 642, 925, 2377, 65, 66, 2982, 297, 880, 2466, 1827, 7053, 3088, 264, 881, 547, 510, 9800, 2914, 3088, 264, 2377, 3088, 5270, 402, 291, 2914, 266, 65, 13, 65, 9536, 1233, 96, 22, 98, 96, 25, 98, 65, 6315, 2504, 2113, 268, 1698, 297, 65, 13, 65, 71, 6746, 1407, 96, 7053, 98, 65, 14, 2, 1, 1227, 286, 264, 56, 1840, 9265, 6869, 291, 1091, 5931, 5931, 266, 25, 4166, 268, 1227, 1195, 264, 1091, 9265, 6869, 291, 1091, 5931, 5931, 266, 763, 4166, 268, 1091, 5931, 5931, 1227, 286, 6017, 26, 1454, 6869, 264, 2392, 570, 6017, 3489, 1454, 268, 687, 22, 1454, 6869, 3608, 2914, 9979, 264, 1365, 56, 1840, 589, 1091, 5931, 5931, 407, 4129, 328, 2042, 9979, 474, 4, 3921, 264, 759, 264, 6974, 515, 2579, 3614, 2319, 268, 2344, 291, 644, 264, 56, 1840, 304, 1227, 286, 9265, 6869, 291, 1091, 5931, 5931, 266, 1419, 4166, 264, 1227, 1195, 9265, 4847, 763, 4166, 268, 5931, 5931, 1227, 286, 6017, 26, 1454, 264, 2392, 570, 4896, 1227, 1195, 6017, 3489, 1454, 268, 667, 1454, 6869, 2914, 9979, 268, 472, 56, 1840, 589, 1091, 5931, 5931, 407, 4129, 328, 2042, 9979, 368, 2415, 474, 1280, 264, 308, 515, 405, 4787, 667, 3872, 1970, 268, 1227, 286, 282, 1227, 1195, 390, 587, 856, 264, 8253, 500, 1506, 567, 729, 9979, 1599, 368, 2415, 474, 1365, 1227, 286, 2472, 264, 5931, 5931, 6017, 26, 1454, 6869, 268, 520, 56, 1840, 291, 3138, 1419]}\n",
      "{'tokens': [2415, 474, 1365, 1227, 286, 2472, 264, 5931, 5931, 6017, 26, 1454, 6869, 268, 520, 56, 1840, 291, 3138, 1419, 4166, 264, 1370, 56, 1840, 1227, 286, 9265, 1651, 26, 7077, 25, 368, 2415, 474, 746, 1651, 26, 6659, 25, 34, 975, 1454, 268, 368, 2415, 474, 1242, 1227, 1195, 1970, 291, 5931, 5931, 6017, 3489, 1454, 264, 56, 1840, 291, 3138, 763, 4166, 264, 1370, 56, 1840, 1227, 1195, 6017, 3489, 6659, 23, 34, 6571, 1454, 268, 368, 1052, 474, 2993, 264, 3499, 856, 56, 1840, 282, 5931, 5931, 6177, 4352, 763, 570, 8913, 6017, 2042, 1454, 6869, 264, 1242, 2893, 567, 1091, 8564, 9979, 264, 1108, 693, 1599, 833, 268, 1280, 264, 5931, 5931, 7174, 570, 8913, 6017, 2042, 1454, 1460, 474, 1227, 286, 26, 1454, 264, 1227, 1195, 3489, 1454, 264, 8913, 26, 16, 3489, 34, 2529, 1454, 268, 56, 1840, 2472, 264, 1227, 286, 975, 1454, 264, 1227, 1195, 6571, 1454, 264, 8913, 975, 16, 6571, 34, 25, 27, 1454, 268, 1242, 667, 1454, 6869, 2914, 9979, 264, 1370, 56, 1840, 4402, 9979, 291, 25, 27, 6659, 2914, 264, 5931, 5931, 291, 2529, 6659, 2914, 268, 1242, 567, 56, 1840, 589, 5931, 5931, 407, 2042, 9979, 264, 1651, 662, 25, 27, 6659, 2914, 643, 18, 662, 2529, 6659, 2914, 643, 34, 662, 25, 27, 18, 2529, 643, 6659, 2914, 34, 4994, 6659, 2914, 34, 26, 27, 758, 9979, 268, 368, 1052, 474, 1654, 264, 380, 383, 264, 380, 1290, 264, 3355, 1471, 4540, 856, 1210, 328, 474, 2734, 1108, 1239, 286, 2424, 268, 1227, 286, 1970, 297, 5931, 5931, 26, 1454, 264, 56, 1840, 291, 25, 4166, 264, 1370, 26, 6659, 25, 34, 975, 264, 368, 268, 1227, 1195, 5931, 5931, 3489, 1454, 264, 56, 1840, 763, 4166, 264, 3489, 6659, 23, 34, 6571, 264, 8187, 268, 729, 6869, 363, 2472, 264, 5931, 5931, 26, 16, 3489, 34, 2529, 264, 56, 1840, 975, 16, 6571, 34, 25, 27, 264, 368, 266, 268, 1370, 56, 1840, 589, 5931, 5931, 4401, 328, 25, 27, 18, 2529, 34, 4994, 1454, 6869, 268, 667, 1454, 2914, 1865, 264, 1370, 4994, 6659, 2914, 34, 26, 27, 758, 1865, 268, 331, 3703, 2761, 268, 1132, 3467, 7686, 549, 264, 2344, 291, 390, 763, 570, 2893, 856, 1599, 833, 1242, 5295, 474, 1279, 264, 1227, 286, 746, 3872, 264, 56, 1840, 589, 5931, 5931, 4401, 328, 2042, 264, 1242, 1227, 1195, 746, 3872, 479, 4401, 328, 2042, 264, 1108, 1395, 763, 4359, 1599, 833, 459, 1506, 474, 1227, 286, 2472, 264, 56, 1840, 1183, 975, 1454, 264, 5931, 5931, 1183, 26, 1454, 264, 1370, 975, 18, 26, 34, 1947, 1454, 268, 1227, 1195, 56, 1840, 1183, 6571, 1454, 264, 5931, 5931, 3489, 1454, 264, 6571, 18, 3489, 34, 3489, 1454, 268, 8913, 4401, 266, 6869, 8835, 1947, 16, 3489, 34, 4994, 1454, 264, 282, 2237, 3961, 4777, 268, 1370, 8803, 1662, 567, 1682, 4994, 1454, 264, 7077, 2914, 1651, 26, 27, 758, 9979, 268, 8421, 8187, 264, 368, 2415, 474, 549, 636, 4810, 368, 266, 268, 746, 1960, 1698, 3703, 26, 27, 758, 9979, 264]}\n",
      "{'tokens': [8187, 264, 368, 2415, 474, 549, 636, 4810, 368, 266, 268, 746, 1960, 1698, 3703, 26, 27, 758, 9979, 264, 316, 2199, 4181, 1661, 341, 65, 71, 6746, 1407, 96, 26, 27, 758, 98, 268, 2591, 264, 1108, 2579, 750, 1290, 2344, 3586, 6925, 2438, 2922, 268, 1279, 264, 3586, 549, 2344, 755, 608, 2392, 570, 607, 5603, 1227, 1195, 3361, 1195, 570, 474, 1279, 264, 1227, 1195, 3361, 1195, 570, 291, 1227, 876, 474, 2591, 2344, 727, 1227, 1195, 1970, 1159, 2173, 6588, 2392, 570, 264, 4896, 2344, 755, 701, 962, 291, 1227, 286, 282, 1227, 1195, 763, 570, 264, 1370, 5931, 5931, 1227, 1195, 6017, 3489, 1454, 268, 1370, 7491, 268, 2209, 264, 9979, 856, 1107, 321, 390, 8092, 856, 474, 1279, 264, 1227, 286, 266, 1599, 833, 282, 1227, 1195, 2893, 856, 9979, 264, 1108, 5295, 474, 2591, 1209, 871, 569, 696, 3120, 268, 1279, 264, 1227, 286, 4401, 328, 1947, 1454, 264, 1212, 1947, 6659, 2914, 34, 24, 2048, 1046, 1227, 1195, 4401, 328, 3489, 1454, 264, 1212, 3489, 6659, 2914, 34, 6571, 758, 264, 8281, 1640, 24, 2048, 16, 6571, 758, 34, 26, 27, 758, 268, 1370, 2847, 5455, 775, 1209, 599, 2864, 268, 8421, 291, 2761, 268, 5, 1227, 286, 264, 56, 1840, 266, 5931, 5931, 6017, 26, 1454, 6869, 264, 56, 1840, 1183, 1524, 3138, 25, 4166, 264, 911, 56, 1840, 6017, 297, 4768, 64, 1743, 4768, 89, 7412, 2002, 642, 3832, 4768, 1641, 65, 3234, 96, 1454, 98, 14, 4768, 66, 226, 1227, 1195, 264, 5931, 5931, 6017, 3489, 1454, 264, 56, 1840, 1183, 1524, 3138, 23, 4166, 264, 911, 56, 1840, 6017, 297, 4768, 64, 8510, 4768, 89, 7412, 1302, 642, 1302, 27, 4768, 1641, 65, 3234, 96, 1454, 98, 14, 4768, 66, 1208, 729, 505, 1468, 1040, 226, 5931, 5931, 763, 570, 1022, 1183, 297, 65, 13, 1743, 1559, 8510, 642, 8137, 4768, 1641, 65, 3234, 96, 1454, 98, 14, 4768, 14, 1040, 1697, 1840, 763, 570, 1022, 1183, 297, 65, 13, 3832, 1559, 1302, 27, 642, 2002, 27, 4768, 1641, 65, 3234, 96, 1454, 98, 14, 4768, 14, 1697, 1840, 589, 5931, 5931, 4401, 266, 6869, 4575, 297, 4768, 64, 2002, 27, 1040, 8137, 642, 1302, 29, 4768, 1641, 65, 3234, 96, 1454, 98, 14, 4768, 66, 226, 667, 1454, 6869, 2914, 9979, 264, 911, 407, 4129, 266, 9979, 341, 297, 4768, 64, 1302, 29, 4768, 89, 7412, 8598, 642, 1743, 27, 758, 4768, 1641, 65, 3234, 96, 9979, 98, 14, 4768, 66, 226, 1960, 1698, 341, 297, 4768, 71, 6746, 1407, 96, 26, 27, 758, 98, 2, 1, 5264, 56, 540, 83, 94, 639, 403, 1437, 4593, 990, 1240, 4368, 4484, 6869, 268, 403, 6164, 403, 4715, 4715, 1870, 4484, 264, 6164, 403, 5931, 5931, 30, 4484, 264, 569, 6164, 403, 507, 4715, 28, 4484, 268, 403, 5337, 2042, 4484, 6869, 474, 4, 3921, 264, 759, 264, 6974, 515, 9177, 268, 2734, 2579, 1154, 286, 2424, 2344, 268, 2344, 291, 644, 264, 56, 540, 83, 94, 5264, 639, 1437, 4593, 990, 1240, 4368, 4484, 6869, 268, 1242, 403, 2893, 6164]}\n",
      "{'tokens': [644, 264, 56, 540, 83, 94, 5264, 639, 1437, 4593, 990, 1240, 4368, 4484, 6869, 268, 1242, 403, 2893, 6164, 1664, 4715, 4715, 1870, 4484, 264, 5931, 5931, 30, 4484, 264, 1638, 507, 4715, 28, 4484, 268, 1374, 321, 567, 403, 5337, 2042, 4484, 6869, 268, 759, 264, 2319, 3713, 291, 5260, 9083, 807, 264, 4279, 321, 1818, 1818, 2579, 405, 4787, 264, 1062, 951, 4540, 9344, 1210, 268, 1280, 264, 56, 540, 83, 94, 9533, 990, 4765, 6869, 4484, 8835, 4368, 4484, 268, 368, 2415, 474, 1242, 403, 4723, 328, 2097, 1802, 276, 254, 297, 4715, 4715, 299, 5931, 5931, 282, 507, 4715, 268, 3499, 856, 403, 8913, 6164, 2042, 4484, 264, 1242, 316, 729, 363, 6895, 636, 729, 1894, 264, 8735, 1651, 403, 1638, 2042, 4484, 268, 746, 1280, 264, 8270, 839, 567, 404, 403, 8913, 1835, 266, 6869, 4484, 363, 268, 2344, 727, 644, 6164, 4715, 4715, 1870, 4484, 264, 5931, 5931, 30, 4484, 264, 507, 4715, 28, 4484, 268, 1370, 729, 4156, 1870, 1559, 30, 1559, 28, 4484, 740, 390, 7494, 268, 636, 7946, 308, 515, 567, 368, 328, 268, 839, 567, 1870, 459, 30, 5357, 474, 1870, 459, 30, 3759, 5232, 264, 368, 2415, 474, 8253, 4684, 28, 264, 5232, 459, 28, 3759, 4994, 268, 1370, 729, 4156, 4994, 4484, 740, 390, 404, 5241, 268, 1325, 264, 8735, 1651, 880, 3812, 4368, 4484, 6895, 4994, 4484, 268, 2993, 264, 856, 4368, 836, 4994, 268, 2087, 2730, 858, 2703, 690, 9083, 268, 1132, 3467, 917, 775, 474, 1279, 264, 390, 1496, 836, 268, 1132, 839, 567, 4368, 836, 975, 3759, 2856, 264, 8253, 836, 29, 264, 1325, 2856, 836, 29, 3759, 3522, 474, 1325, 2472, 264, 1698, 3703, 3522, 4484, 268, 368, 1052, 474, 2591, 264, 4535, 871, 1108, 1239, 286, 2424, 264, 1062, 667, 1818, 599, 8187, 268, 1280, 264, 390, 7494, 266, 2097, 1141, 291, 1870, 299, 30, 282, 28, 268, 459, 1506, 9750, 474, 1870, 459, 30, 3455, 291, 5232, 264, 5609, 5232, 459, 28, 291, 4994, 268, 8187, 268, 729, 4156, 4368, 4484, 6895, 4994, 4484, 264, 3390, 1524, 3522, 4484, 268, 746, 2761, 1698, 1107, 1651, 3522, 474, 1132, 3467, 549, 308, 4540, 3774, 3243, 328, 474, 1279, 2344, 727, 2443, 3467, 917, 2438, 2922, 474, 1279, 264, 702, 6869, 4484, 1107, 1682, 1191, 264, 1132, 3467, 549, 2853, 856, 474, 566, 2344, 727, 2173, 6588, 6164, 2097, 1191, 322, 264, 6177, 4057, 264, 1370, 459, 1506, 8676, 266, 268, 1370, 871, 7491, 268, 1132, 264, 3467, 549, 2344, 755, 608, 8597, 607, 2968, 7287, 1678, 474, 1279, 56, 540, 83, 94, 3467, 630, 1183, 3243, 1132, 2209, 907, 3243, 474, 2344, 727, 811, 3528, 264, 894, 644, 328, 592, 4715, 4715, 299, 5931, 5931, 282, 507, 4715, 268, 1370, 2742, 334, 3190, 917, 1100, 1970, 264, 894, 385, 856, 1835, 266, 331, 876, 1409, 4402, 282, 264, 1242, 639, 729, 363, 326, 6895, 268, 1108, 567, 286, 2424, 8281, 268, 1870, 16, 30, 34, 5232, 264, 5232, 16, 28, 34, 4994, 1046, 4368, 18, 4994, 264, 340, 1325, 567, 297, 4368, 18]}\n",
      "{'tokens': [30, 34, 5232, 264, 5232, 16, 28, 34, 4994, 1046, 4368, 18, 4994, 264, 340, 1325, 567, 297, 4368, 18, 975, 34, 2856, 264, 2856, 18, 29, 34, 3522, 268, 7686, 874, 264, 4368, 18, 4994, 264, 549, 316, 2160, 363, 429, 268, 1279, 264, 4994, 1373, 2089, 1599, 23, 264, 1370, 4368, 18, 2089, 34, 1947, 264, 1242, 4684, 23, 264, 1466, 3522, 268, 1325, 3629, 775, 599, 1466, 450, 3120, 1209, 264, 2198, 1658, 268, 1370, 1349, 7594, 871, 1651, 4368, 6895, 662, 1870, 16, 30, 16, 28, 643, 34, 4994, 264, 3390, 3522, 4484, 268, 3390, 1524, 3522, 4484, 6869, 268, 368, 2415, 474, 5, 56, 540, 83, 94, 9533, 309, 4368, 4484, 6869, 268, 403, 4723, 4715, 4715, 1870, 4484, 264, 5931, 5931, 30, 4484, 264, 507, 4715, 28, 4484, 268, 1280, 856, 403, 8913, 1835, 266, 6869, 4484, 363, 297, 4768, 64, 1870, 1559, 3210, 1559, 2991, 642, 1302, 29, 4768, 4768, 3234, 96, 4484, 98, 268, 4768, 66, 226, 1242, 316, 729, 363, 4368, 4484, 6895, 1835, 266, 4994, 4484, 297, 4768, 64, 4368, 1040, 1302, 29, 642, 925, 28, 4768, 4768, 3234, 96, 4484, 98, 268, 4768, 66, 226, 911, 264, 56, 540, 83, 94, 5337, 65, 71, 6746, 1407, 96, 3522, 98, 4484, 6869, 268, 2, 1, 2817, 5676, 751, 1481, 2658, 1834, 2590, 510, 264, 7004, 7562, 25, 3752, 9115, 264, 2004, 1886, 2729, 1529, 727, 3010, 25, 3752, 9115, 268, 304, 2457, 1227, 727, 264, 2817, 5676, 1637, 1723, 328, 26, 3752, 9115, 268, 687, 331, 1227, 2817, 5676, 282, 3138, 3490, 2388, 328, 24, 3752, 9115, 264, 5337, 2042, 3752, 474, 4, 3921, 264, 2734, 2579, 3614, 2319, 268, 2817, 5676, 304, 2590, 2968, 264, 7004, 7562, 25, 3752, 9115, 264, 2004, 1886, 2729, 1529, 727, 3010, 25, 3752, 264, 368, 2415, 268, 746, 1280, 8270, 567, 1290, 1091, 7380, 729, 4156, 2042, 3752, 9115, 268, 7004, 25, 3752, 4684, 1886, 2729, 1529, 266, 25, 3752, 264, 3703, 25, 16, 25, 3759, 29, 3752, 268, 368, 2415, 474, 1242, 2344, 644, 2457, 1227, 727, 1091, 1637, 6402, 26, 3752, 268, 1370, 9173, 729, 4156, 6279, 29, 3752, 1108, 4684, 523, 1308, 266, 26, 3752, 264, 3703, 29, 16, 26, 34, 3489, 3752, 268, 368, 2415, 474, 2993, 264, 2817, 5676, 282, 3138, 3490, 304, 2457, 1227, 2388, 328, 24, 3752, 268, 746, 8735, 871, 1651, 3489, 3752, 6895, 2388, 3243, 266, 24, 3752, 264, 4896, 3489, 18, 24, 34, 1175, 3752, 268, 1370, 8735, 3703, 1175, 3752, 1052, 474, 2591, 1654, 264, 3355, 1471, 4540, 9344, 1210, 328, 474, 1279, 264, 1091, 1723, 8905, 2388, 3243, 2236, 3467, 3796, 1522, 474, 2344, 727, 644, 608, 304, 2457, 1227, 727, 607, 6402, 26, 3752, 264, 1242, 2388, 328, 24, 3752, 264, 1370, 549, 1723, 282, 2388, 3792, 291, 863, 8285, 266, 264, 1370, 3796, 549, 334, 783, 268, 4896, 644, 264, 8803, 789, 1647, 1308, 266, 264, 729, 4156, 880, 3812, 29, 3752, 264, 4684, 26, 3752, 264, 1108, 6895, 2388, 3243, 266, 24, 3752, 264, 1209, 1682, 2761, 268, 1370, 1349]}\n",
      "{'tokens': [3752, 264, 4684, 26, 3752, 264, 1108, 6895, 2388, 3243, 266, 24, 3752, 264, 1209, 1682, 2761, 268, 1370, 1349, 8735, 3703, 1175, 3752, 368, 1052, 474, 1132, 3467, 549, 1886, 2729, 1529, 282, 7004, 3807, 9115, 2217, 1723, 2968, 1159, 776, 328, 746, 26, 3752, 3807, 474, 2591, 2344, 727, 9380, 562, 4787, 264, 2817, 5676, 751, 1481, 2658, 1834, 2590, 510, 264, 7004, 7562, 25, 3752, 264, 1886, 2729, 1529, 309, 25, 3752, 264, 2968, 331, 1227, 1637, 6402, 26, 3752, 268, 1370, 880, 3812, 29, 3752, 291, 390, 587, 266, 264, 2968, 1637, 459, 266, 26, 3752, 264, 1370, 729, 4156, 3489, 3752, 268, 1242, 2388, 328, 24, 3752, 264, 3390, 1175, 3752, 268, 871, 8187, 2415, 474, 1654, 264, 3467, 549, 4540, 1211, 1210, 328, 474, 1279, 608, 751, 1481, 2658, 1834, 2590, 510, 607, 549, 795, 266, 1651, 2457, 1227, 727, 1856, 754, 2590, 474, 7120, 8733, 1191, 2590, 474, 2344, 727, 644, 608, 2817, 5676, 751, 1481, 2658, 1834, 2590, 510, 264, 7004, 309, 25, 3752, 264, 1886, 2729, 1529, 309, 25, 3752, 2129, 1242, 608, 2457, 1227, 727, 1637, 1723, 328, 26, 3752, 2129, 1370, 880, 3812, 29, 3752, 291, 6054, 2590, 2515, 1894, 264, 2968, 1637, 1308, 266, 26, 3752, 291, 331, 1227, 574, 3361, 1195, 754, 1723, 268, 1370, 729, 4156, 29, 16, 26, 34, 3489, 264, 1242, 2388, 328, 24, 3752, 264, 3390, 1175, 3752, 268, 368, 266, 264, 3703, 1325, 268, 5, 2817, 5676, 7380, 304, 7004, 282, 1886, 2729, 1529, 4156, 25, 16, 25, 34, 29, 3752, 9115, 268, 2457, 1227, 1091, 1637, 1723, 328, 26, 3752, 264, 6578, 729, 4156, 29, 16, 26, 34, 3489, 3752, 268, 2968, 3490, 2388, 328, 24, 3752, 264, 911, 8735, 9115, 1894, 341, 3489, 18, 24, 34, 1175, 3752, 268, 1698, 297, 65, 71, 6746, 1407, 96, 1175, 98, 2, 1, 394, 4020, 2991, 226, 4166, 291, 2607, 5136, 226, 17, 7377, 8835, 1641, 226, 7372, 38, 19, 2693, 30, 39, 19, 2002, 6005, 22, 40, 19, 3210, 29, 4, 3921, 264, 6974, 321, 9177, 264, 2344, 291, 644, 394, 4020, 28, 4166, 3759, 27, 5136, 264, 1242, 321, 616, 4213, 2827, 268, 2827, 291, 38, 19, 8397, 264, 39, 19, 25, 6005, 22, 264, 40, 19, 30, 29, 268, 308, 515, 2579, 645, 645, 7938, 268, 1280, 264, 2344, 871, 960, 2589, 610, 363, 4298, 268, 2344, 755, 608, 394, 363, 607, 1232, 617, 341, 93, 264, 1365, 744, 448, 516, 264, 636, 4020, 28, 4166, 1651, 28, 7077, 93, 3759, 27, 5136, 268, 2199, 4016, 3703, 28, 93, 642, 2607, 5136, 268, 746, 2993, 3499, 464, 636, 4298, 264, 693, 93, 5152, 268, 464, 636, 4298, 2472, 264, 871, 1651, 1395, 763, 1574, 599, 5001, 28, 264, 1466, 93, 3759, 27, 5136, 5001, 28, 3961, 268, 9173, 3499, 856, 27, 5136, 5001, 28, 5357, 268, 331, 1818, 2730, 2149, 567, 1132, 537, 567, 264, 2591, 308, 3790, 3760, 264, 515, 2579, 567, 264, 1214, 404, 1210, 268, 2734, 6122, 856, 27, 5136, 5001, 28, 268, 1280, 264, 28, 2289, 3965, 291]}\n",
      "{'tokens': [567, 264, 1214, 404, 1210, 268, 2734, 6122, 856, 27, 5136, 5001, 28, 268, 1280, 264, 28, 2289, 3965, 291, 26, 3504, 264, 368, 2415, 474, 1007, 28, 2289, 29, 291, 26, 27, 264, 9667, 459, 302, 21, 1651, 26, 3504, 268, 1242, 27, 5136, 6895, 26, 3504, 2472, 264, 3390, 27, 24, 268, 2993, 264, 28, 2289, 30, 291, 27, 24, 264, 1370, 9173, 956, 1651, 3965, 459, 30, 264, 4896, 8397, 268, 1370, 27, 5136, 5001, 28, 3759, 8397, 264, 746, 636, 363, 93, 1651, 8397, 264, 8529, 2827, 38, 368, 1052, 474, 2591, 264, 1706, 3530, 1357, 856, 8187, 264, 8270, 1108, 1239, 286, 2424, 268, 1279, 264, 316, 8397, 7077, 28, 264, 3614, 1209, 5850, 27, 5136, 268, 839, 567, 3965, 2289, 28, 291, 26, 3504, 264, 1108, 4684, 30, 2289, 28, 266, 27, 24, 264, 26, 3504, 459, 27, 24, 3759, 27, 5136, 268, 8187, 264, 1325, 1209, 8676, 266, 268, 746, 2827, 38, 291, 2761, 268, 2591, 264, 2734, 1108, 3614, 2827, 755, 917, 2827, 3467, 549, 2244, 595, 234, 1970, 268, 2827, 39, 291, 25, 6005, 22, 264, 331, 1415, 531, 5075, 264, 1007, 2344, 3807, 1209, 291, 27, 5136, 264, 520, 880, 3812, 8835, 93, 264, 1370, 687, 933, 8175, 316, 27, 5136, 7077, 28, 2472, 264, 3455, 291, 25, 6005, 22, 264, 566, 2344, 644, 1524, 4020, 28, 4166, 3759, 27, 5136, 264, 1370, 636, 2827, 39, 2503, 2887, 5877, 2827, 264, 681, 3082, 6602, 537, 1272, 1395, 8464, 282, 7020, 9344, 933, 328, 268, 1279, 264, 6334, 1278, 2402, 567, 27, 5136, 2289, 28, 1466, 25, 6005, 22, 264, 1242, 616, 39, 264, 566, 1824, 8701, 264, 871, 316, 7020, 3337, 8464, 268, 520, 2827, 40, 291, 30, 29, 264, 308, 1972, 2982, 1290, 268, 30, 29, 7077, 28, 5357, 474, 28, 2289, 4285, 291, 27, 2089, 264, 28, 2289, 29, 291, 26, 27, 264, 1370, 27, 2089, 459, 26, 27, 3759, 27, 29, 27, 264, 3062, 589, 27, 5136, 418, 1393, 264, 1370, 40, 2827, 334, 368, 268, 911, 2761, 1698, 4329, 291, 38, 2827, 8397, 268, 5, 321, 8364, 4020, 28, 4166, 3759, 27, 5136, 264, 400, 617, 636, 4575, 65, 13, 2843, 4768, 6315, 744, 448, 516, 309, 4298, 297, 4768, 64, 2991, 93, 642, 2607, 5136, 4768, 66, 2497, 763, 1574, 863, 5001, 28, 297, 4768, 64, 2843, 642, 4768, 9536, 1233, 96, 27, 5136, 98, 96, 28, 98, 4768, 66, 8305, 856, 297, 4768, 64, 2991, 4768, 89, 7412, 2693, 30, 642, 2607, 5136, 4768, 66, 226, 2982, 300, 781, 264, 8397, 291, 2144, 2113, 266, 363, 268, 226, 2827, 39, 662, 25, 6005, 22, 643, 291, 27, 5136, 7077, 28, 3961, 264, 2682, 2289, 1084, 2244, 595, 234, 266, 1920, 1046, 2827, 40, 662, 30, 29, 643, 266, 28, 4166, 341, 27, 29, 27, 264, 544, 27, 5136, 334, 1035, 268, 911, 264, 1658, 1698, 341, 297, 4768, 71, 6746, 1407, 96, 38, 98, 2]}\n",
      "{'tokens': [1, 6403, 4443, 427, 3730, 404, 6303, 4484, 6869, 268, 381, 2167, 8998, 264, 1091, 1159, 838, 3138, 2311, 719, 107, 3730, 3107, 1870, 4484, 5500, 6869, 264, 838, 3138, 5486, 3730, 3107, 26, 4484, 3856, 6869, 264, 838, 394, 5154, 1600, 3730, 3107, 25, 4484, 8570, 6869, 268, 6403, 4443, 2675, 3730, 404, 2042, 4484, 6869, 474, 4, 3921, 264, 759, 264, 6974, 321, 1038, 6403, 4443, 2675, 3730, 404, 2042, 4484, 6869, 1522, 268, 2344, 644, 6403, 4443, 427, 3730, 404, 8913, 6303, 4484, 6869, 268, 1242, 1091, 1159, 3730, 3005, 592, 1191, 322, 264, 3499, 839, 567, 1091, 8913, 1159, 3730, 3107, 2042, 264, 8253, 316, 729, 363, 6895, 1159, 3730, 3403, 1894, 264, 1651, 1091, 2675, 3730, 4057, 268, 2734, 2579, 3614, 2344, 727, 2443, 268, 1280, 264, 2344, 3528, 6403, 4443, 838, 3138, 2311, 719, 107, 3730, 3107, 1870, 4484, 5500, 6869, 268, 2087, 2443, 291, 1870, 4484, 264, 368, 2415, 474, 1242, 838, 3138, 5486, 3730, 328, 26, 4484, 3856, 6869, 264, 4896, 26, 4484, 268, 5609, 291, 838, 394, 5154, 1600, 3730, 328, 25, 4484, 8570, 6869, 264, 291, 25, 4484, 268, 746, 331, 2097, 1141, 459, 1506, 871, 1651, 1091, 2167, 1159, 3730, 3403, 729, 482, 268, 746, 308, 4571, 1290, 1159, 3730, 3403, 729, 363, 2415, 268, 1870, 459, 26, 264, 3759, 3522, 264, 8253, 4684, 25, 268, 3522, 459, 25, 3759, 5232, 368, 1052, 474, 368, 266, 264, 1370, 6403, 4443, 1159, 3730, 404, 5241, 5232, 4484, 6869, 268, 1365, 729, 4156, 6303, 4484, 427, 3730, 404, 264, 1374, 8735, 1651, 6303, 6895, 5232, 3961, 268, 2993, 856, 6303, 836, 5232, 5357, 268, 3921, 264, 331, 871, 334, 1173, 264, 6303, 6895, 975, 3759, 3489, 264, 1108, 6895, 22, 2472, 1651, 1870, 474, 1132, 644, 264, 5232, 4684, 2042, 3759, 6303, 474, 5232, 459, 1175, 291, 6824, 264, 1108, 4684, 23, 1651, 6303, 264, 1370, 729, 4156, 1870, 268, 1370, 871, 9845, 1870, 4484, 427, 3730, 268, 2591, 264, 308, 515, 1108, 3550, 1290, 1496, 264, 1062, 951, 4540, 567, 1210, 328, 268, 1280, 264, 2097, 1564, 2278, 8005, 1870, 299, 26, 299, 25, 268, 459, 1506, 2472, 297, 1870, 16, 26, 34, 3522, 264, 8187, 268, 1242, 3522, 16, 25, 34, 5232, 264, 636, 479, 368, 268, 729, 427, 3730, 404, 1524, 6303, 4484, 264, 1370, 8735, 1651, 6303, 18, 5232, 34, 1870, 268, 8421, 8676, 266, 268, 3467, 549, 2344, 5413, 917, 4097, 473, 115, 1132, 2438, 2922, 474, 1279, 264, 1107, 1223, 3730, 3403, 6869, 9292, 567, 304, 729, 363, 727, 474, 2344, 727, 644, 1524, 608, 381, 2167, 8998, 264, 1091, 1159, 838, 3138, 2311, 719, 107, 3730, 3107, 1870, 4484, 5500, 6869, 264, 838, 3138, 5486, 3730, 3107, 26, 4484, 3856, 6869, 264, 838, 394, 5154, 1600, 3730, 3107, 25, 4484, 8570, 6869, 1809, 331, 2198, 331, 2097, 1141, 1682, 928, 3730, 3403, 264, 1376, 729, 1205, 8835, 6303, 4484, 268, 1370, 871, 951, 533, 264, 2402, 5295, 1242, 561, 836, 2678, 268, 1370, 3681, 291, 6403, 4443, 2675, 3730, 404, 1870, 4484, 6869, 268, 5, 6403]}\n",
      "{'tokens': [5295, 1242, 561, 836, 2678, 268, 1370, 3681, 291, 6403, 4443, 2675, 3730, 404, 1870, 4484, 6869, 268, 5, 6403, 4443, 427, 3730, 404, 6303, 4484, 6869, 268, 1091, 1159, 3730, 3403, 1894, 9928, 297, 2311, 719, 107, 1870, 4484, 264, 5486, 26, 4484, 264, 5154, 1600, 25, 4484, 268, 1280, 856, 928, 3730, 3403, 729, 363, 297, 65, 64, 1870, 1559, 1743, 1559, 2002, 642, 1302, 22, 4768, 3234, 96, 4484, 98, 65, 66, 1242, 316, 729, 1161, 6895, 928, 3730, 3403, 1894, 297, 65, 64, 6303, 1040, 1302, 22, 642, 4429, 4768, 3234, 96, 4484, 98, 65, 66, 911, 264, 6403, 4443, 2675, 3730, 3403, 6869, 1894, 341, 65, 71, 6746, 1407, 96, 1870, 98, 268, 2, 1, 5847, 415, 2001, 880, 886, 291, 925, 3504, 226, 809, 264, 432, 886, 291, 925, 975, 226, 809, 264, 1348, 886, 328, 662, 643, 65, 5315, 4, 3921, 264, 6974, 515, 567, 9577, 415, 2001, 1348, 886, 5072, 128, 390, 589, 268, 2344, 291, 644, 880, 886, 3053, 21, 809, 264, 1374, 1712, 291, 8709, 809, 264, 1500, 1348, 886, 5072, 128, 390, 589, 268, 746, 871, 1662, 567, 1460, 474, 2734, 2579, 645, 645, 268, 1280, 264, 1348, 886, 328, 8722, 1460, 474, 880, 886, 6895, 432, 886, 368, 2415, 268, 746, 3053, 21, 809, 6895, 8709, 809, 264, 3759, 3356, 809, 268, 4896, 644, 264, 1712, 1348, 328, 3356, 809, 268, 368, 2415, 264, 746, 636, 3356, 809, 291, 1348, 886, 8245, 264, 1370, 427, 567, 636, 1348, 886, 8245, 3013, 880, 886, 5072, 128, 390, 589, 268, 746, 2521, 390, 589, 266, 856, 3703, 1348, 886, 5712, 5001, 880, 886, 264, 1242, 7077, 2512, 5315, 4896, 662, 3356, 20, 3053, 21, 643, 6659, 2512, 5315, 368, 2415, 474, 9173, 308, 515, 839, 567, 3356, 5001, 3053, 21, 3759, 2042, 268, 3921, 264, 3053, 21, 390, 503, 3356, 264, 1188, 390, 2472, 264, 3556, 282, 390, 1593, 599, 4442, 3356, 7782, 268, 3356, 5001, 3356, 291, 22, 264, 3053, 21, 5001, 3356, 291, 25, 268, 1370, 1651, 22, 20, 25, 268, 22, 20, 25, 3759, 21, 19, 2856, 264, 1242, 7077, 2512, 10, 1651, 2856, 5315, 1370, 1348, 886, 328, 2856, 10, 368, 1052, 474, 1654, 264, 1654, 264, 3355, 1471, 4540, 9344, 1210, 328, 474, 1279, 264, 3467, 549, 871, 316, 1348, 4989, 5001, 880, 886, 264, 1794, 432, 886, 474, 1279, 264, 6946, 1278, 1166, 258, 1210, 264, 566, 2761, 3703, 1348, 886, 5001, 880, 886, 368, 2415, 268, 1007, 2521, 390, 589, 3582, 291, 561, 1106, 880, 3812, 1712, 1551, 266, 268, 1279, 264, 880, 886, 291, 9048, 264, 1370, 316, 1348, 8245, 5001, 880, 886, 1198, 368, 268, 1279, 264, 880, 886, 3053, 21, 264, 1348, 886, 3356, 809, 264, 1370, 3356, 20, 3053, 21, 34, 2856, 6545, 368, 266, 268, 746, 687, 432, 886, 291, 8709, 264, 1370, 871, 8187, 2470, 268, 1132, 264, 6334, 1278, 316, 1348, 886, 2515, 1712, 4571, 264, 1279, 316, 3356, 5001, 8709, 264, 566, 746, 1415, 531, 5782, 368, 266, 268, 1279, 264, 1325, 1466, 1524, 6303, 19, 24]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# 文件路径\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/pretrain_data/pretrain.jsonl\"\n",
    "\n",
    "# 统计文件总行数\n",
    "def count_lines(file_path):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        return sum(1 for _ in file)\n",
    "\n",
    "# 使用生成器读取前 n 行数据\n",
    "def load_first_n_lines(file_path, n=10):\n",
    "    with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "        for i, line in enumerate(file):\n",
    "            if i >= n:  # 只读取前n行\n",
    "                break\n",
    "            line = line.strip()  # 去除行首尾的空白字符\n",
    "            if not line:  # 跳过空行\n",
    "                continue\n",
    "            try:\n",
    "                yield json.loads(line)  # 解析每一行的JSON数据\n",
    "            except json.JSONDecodeError:\n",
    "                print(f\"跳过无效行: {line}\")\n",
    "\n",
    "# 获取总行数\n",
    "total_lines = count_lines(file_path)\n",
    "\n",
    "# 读取前10行数据\n",
    "data = list(load_first_n_lines(file_path, n=10))\n",
    "\n",
    "print(f\"成功加载 {len(data)} 条数据, 共计 {total_lines} 条数据\")\n",
    "\n",
    "for d in data:\n",
    "    print(d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "共计:3060824071tokens\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/sft.jsonl\"\n",
    "count = 0\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "      for line in file:\n",
    "            data = json.loads(line.strip())\n",
    "            count += int(data['tokens'])\n",
    "\n",
    "\n",
    "print(f\"共计:{count}tokens\")\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File 1 (tokens < 490): 8364950\n",
      "File 2 (tokens > 2000): 497277\n",
      "File 3 (validate exists): 6797\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/deduped_sft.jsonl\"\n",
    "file_path_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/sft_512.jsonl\"\n",
    "file_path_2 = \"/home/mth/project_llm/mini_llm/data/origin_data/sft_4k.jsonl\"\n",
    "file_path_3 = \"/home/mth/project_llm/mini_llm/data/origin_data/sft_rl.jsonl\"\n",
    "\n",
    "buffer = []\n",
    "count_1 = 0  # 计数存入file_path_1的数量\n",
    "count_2 = 0  # 计数存入file_path_2的数量\n",
    "count_3 = 0  # 计数存入file_path_3的数量\n",
    "\n",
    "k_data = []\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        if data['conversations'][0]['content']:\n",
    "            buffer.append(data)\n",
    "            if len(buffer) > 10000:\n",
    "                for d in buffer:\n",
    "                    if 'validate' in d and d['validate']:\n",
    "                        # with open(file_path_3, 'a', encoding='utf-8') as f:\n",
    "                        #     f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "                        count_3 += 1\n",
    "                    elif d['tokens'] < 490 and \"<think>\" not in d['conversations'][1]['content']:\n",
    "                        # with open(file_path_1, 'a', encoding='utf-8') as f:\n",
    "                        #     f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "                        count_1 += 1\n",
    "                    elif d['tokens'] > 1000 and \"<think>\" not in d['conversations'][1]['content']:\n",
    "                        k_data.append(d)\n",
    "                        count_2 += 1\n",
    "                buffer = []\n",
    "\n",
    "# 处理剩余的buffer\n",
    "if buffer:\n",
    "    for d in buffer:\n",
    "        if 'validate' in d and d['validate']:\n",
    "            # with open(file_path_3, 'a', encoding='utf-8') as f:\n",
    "            #     f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "            count_3 += 1\n",
    "        elif d['tokens'] < 490 and \"<think>\" not in d['conversations'][1]['content']:\n",
    "            # with open(file_path_1, 'a', encoding='utf-8') as f:\n",
    "            #     f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "            count_1 += 1\n",
    "        elif d['tokens'] > 1000 and \"<think>\" not in d['conversations'][1]['content']:\n",
    "            # with open(file_path_2, 'a', encoding='utf-8') as f:\n",
    "            #     f.write(json.dumps(d, ensure_ascii=False) + \"\\n\")\n",
    "            k_data.append(d)\n",
    "            count_2 += 1\n",
    "\n",
    "\n",
    "with open(file_path_2, 'a', encoding='utf-8') as f:\n",
    "    k_data.sort(key=lambda x:x['tokens'])\n",
    "    for data in k_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n",
    "# 打印各文件写入的数量\n",
    "print(f\"File 1 (tokens < 490): {count_1}\")\n",
    "print(f\"File 2 (tokens > 2000): {count_2}\")\n",
    "print(f\"File 3 (validate exists): {count_3}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_0.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_1.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_2.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_3.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_4.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_5.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_6.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_7.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_8.jsonl with 0 lines.\n",
      "Created: /home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_9.jsonl with 0 lines.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "def split_jsonl_file(input_file, num_parts=13):\n",
    "    # 读取原始 JSONL 文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "\n",
    "    # 计算每个子文件包含的行数\n",
    "    total_lines = len(lines)\n",
    "    lines_per_file = total_lines // num_parts\n",
    "    remainder = total_lines % num_parts\n",
    "\n",
    "    # 分割文件并写入子文件\n",
    "    start_idx = 0\n",
    "    for i in range(num_parts):\n",
    "        end_idx = start_idx + lines_per_file + (1 if i < remainder else 0)  # 分配剩余的行\n",
    "        output_file = f\"/home/mth/project_llm/mini_llm/data/pretrain_data/pretrain_{i}.jsonl\"  # 子文件名按数字命名\n",
    "        with open(output_file, 'w', encoding='utf-8') as f_out:\n",
    "            f_out.writelines(lines[start_idx:end_idx])\n",
    "        start_idx = end_idx\n",
    "        print(f\"Created: {output_file} with {end_idx - start_idx} lines.\")\n",
    "\n",
    "# 使用示例\n",
    "input_file = '/home/mth/project_llm/mini_llm/data/pretrain_data/pretrain.jsonl'  # 替换为你的原始文件路径\n",
    "split_jsonl_file(input_file, num_parts=10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "\n",
    "file_path = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_train_reason.jsonl\"\n",
    "file_path_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/TCM_sft_rl.jsonl\"\n",
    "\n",
    "buffer = []\n",
    "k_data = []\n",
    "\n",
    "with open(file_path, mode=\"r\", encoding=\"utf-8\") as file:\n",
    "    for line in file:\n",
    "        data = json.loads(line.strip())\n",
    "\n",
    "        if data['conversations'][0]['content']:\n",
    "            buffer.append(data)\n",
    "            if len(buffer) > 1000:\n",
    "                for d in buffer:\n",
    "                    systme_prompt = {\"role\": \"system\", \"content\": \"user和assistant之间的对话。user提出问题，assistant解决问题。assistant首先在头脑中思考推理过程，然后向用户提供答案。推理过程分别包含在<think> </think>标签中，答案包含在<answer> </answer>标签中，即<think>\\n reasoning process here\\n</think>\\n<answer>\\n answer here \\n</answer>。\"}\n",
    "\n",
    "                    data_1 = d['conversations'][0]['content']\n",
    "                    data_1 = {\"role\": \"user\", \"content\": f\"您必须将您的答案放在<answer> </answer>标签中，即<answer>该问题的答案</answer>。你的最终答案将通过\\\\boxed{{}}标签自动提取出来。\\n以下是具体的问题：\\n{data_1}\"}\n",
    "                    \n",
    "                    data_2 = d['conversations'][1]\n",
    "\n",
    "                    data_2_1 = data_2['content'].split(\"</think>\")[0].strip()\n",
    "                    data_2_2 = data_2['content'].split(\"</think>\")[1].strip()\n",
    "\n",
    "                    data_2 = {\"role\": \"assistant\", \"content\": f\"{data_2_1}\\n</think>\\n<answer>\\n{data_2_2}\\n</answer>\"}\n",
    "\n",
    "                    d['conversations'] = [systme_prompt, data_1, data_2]\n",
    "\n",
    "               \n",
    "                    k_data.append(d)\n",
    "                       \n",
    "                buffer = []\n",
    "\n",
    "# 处理剩余的buffer\n",
    "if buffer:\n",
    "    for d in buffer:\n",
    "        systme_prompt = {\"role\": \"system\", \"content\": \"user和assistant之间的对话。user提出问题，assistant解决问题。assistant首先在头脑中思考推理过程，然后向用户提供答案。推理过程分别包含在<think> </think>标签中，答案包含在<answer> </answer>标签中，即<think>\\n reasoning process here\\n</think>\\n<answer>\\n answer here \\n</answer>。\"}\n",
    "\n",
    "        data_1 = d['conversations'][0]['content']\n",
    "        data_1 = {\"role\": \"user\", \"content\": f\"您必须将您的答案放在<answer> </answer>标签中，即<answer>该问题的答案</answer>。你的最终答案将通过\\\\boxed{{}}标签自动提取出来。\\n以下是具体的问题：\\n{data_1}\"}\n",
    "                    \n",
    "        data_2 = d['conversations'][1]\n",
    "\n",
    "        data_2_1 = data_2['content'].split(\"</think>\")[0].strip()\n",
    "        data_2_2 = data_2['content'].split(\"</think>\")[1].strip()\n",
    "\n",
    "        data_2 = {\"role\": \"assistant\", \"content\": f\"{data_2_1}\\n</think>\\n<answer>\\n{data_2_2}\\n</answer>\"}\n",
    "\n",
    "        d['conversations'] = [systme_prompt, data_1, data_2]\n",
    "\n",
    "               \n",
    "        k_data.append(d)       \n",
    "\n",
    "\n",
    "\n",
    "with open(file_path_1, 'a', encoding='utf-8') as f:\n",
    "    random.shuffle(k_data)\n",
    "    for data in k_data:\n",
    "        f.write(json.dumps(data, ensure_ascii=False) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from sklearn.model_selection import train_test_split\n",
    "import csv\n",
    "import random\n",
    "\n",
    "output_path_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/grpo_tcm_train.jsonl\"\n",
    "output_path_2 = \"/home/mth/project_llm/mini_llm/data/origin_data/grpo_tcm_test.jsonl\"\n",
    "file_path_1 = \"/home/mth/project_llm/mini_llm/data/origin_data/中文数学题目/math23k_test.json\"\n",
    "file_path_2 = \"/home/mth/project_llm/mini_llm/data/origin_data/中文数学题目/math23k_train.json\"\n",
    "file_path_3 = \"/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_train.csv\"\n",
    "\n",
    "buffer = []\n",
    "\n",
    "\n",
    "\n",
    "def write_jsonl_file(data, file_path):\n",
    "    with open(file_path, 'w', encoding='utf-8') as f:\n",
    "        for entry in data:\n",
    "            f.write(json.dumps(entry, ensure_ascii=False) + '\\n')\n",
    "\n",
    "systme_prompt = {\"role\": \"system\", \"content\": \"user和assistant之间的对话。user提出问题，assistant解决问题。assistant首先在头脑中思考推理过程，然后向用户提供答案。推理过程分别包含在<think> </think>标签中，答案包含在<answer> </answer>标签中，即<think>\\n reasoning process here\\n</think>\\n<answer>\\n answer here \\n</answer>。\"}\n",
    "\n",
    "\n",
    "data = []\n",
    "# with open(file_path_1, 'r', encoding='utf-8') as f:\n",
    "#         datas = json.load(f)\n",
    "#         for d in datas:\n",
    "#             data_1 = {\"role\": \"user\", \"content\": f\"您必须将您的答案放在<answer> </answer>标签中，即<answer>该问题的答案</answer>。你的最终答案将通过\\\\boxed{{}}标签自动提取出来。\\n以下是具体的问题：\\n{d['original_text']}\"}\n",
    "#             answer = d['ans']\n",
    "\n",
    "#             data.append({\n",
    "#                 'prompt': [systme_prompt, data_1],\n",
    "#                 'answer':answer\n",
    "#             })\n",
    "\n",
    "# with open(file_path_2, 'r', encoding='utf-8') as f:\n",
    "#         datas = json.load(f)\n",
    "#         for d in datas:\n",
    "#             data_1 = {\"role\": \"user\", \"content\": f\"您必须将您的答案放在<answer> </answer>标签中，即<answer>该问题的答案</answer>。你的最终答案将通过\\\\boxed{{}}标签自动提取出来。\\n以下是具体的问题：\\n{d['original_text']}\"}\n",
    "#             answer = d['ans']\n",
    "\n",
    "#             data.append({\n",
    "#                 'prompt': [systme_prompt, data_1],\n",
    "#                 'answer':answer\n",
    "#             })\n",
    "\n",
    "with open(file_path_3, 'r', encoding='utf-8') as f:\n",
    "        reader = csv.DictReader(f)\n",
    "        for row in reader:\n",
    "            data_1 = {\"role\": \"user\", \"content\": f\"您必须将您的答案放在<answer> </answer>标签中，即<answer>该问题的答案</answer>。你的最终答案将通过\\\\boxed{{}}标签自动提取出来。\\n以下是具体的问题：\\n你是一名专业的中医医生，可以根据病人的病例信息给出病人的辩证。病人的病例：{row['description']}\"}\n",
    "            answer = row['syndrome']\n",
    "\n",
    "            data.append({\n",
    "                'prompt': [systme_prompt, data_1],\n",
    "                'answer':answer\n",
    "            })\n",
    "\n",
    "\n",
    "# 打乱数据\n",
    "random.shuffle(data)\n",
    "\n",
    "    # 划分训练集和测试集，比例为9:1\n",
    "# train_data, test_data = train_test_split(data, test_size=0.1, random_state=42)\n",
    "\n",
    "write_jsonl_file(data, output_path_1)\n",
    "# write_jsonl_file(test_data, output_path_2)\n",
    "             \n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "筛选完成，共保存11171条记录到output.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# 读取原始CSV文件\n",
    "df = pd.read_csv('/home/mth/project_llm/mini_llm/data/origin_data/中医辩证数据集/TCM_syndrome_differentiation_new.csv')\n",
    "\n",
    "# 每个syndrome最多保留300条记录\n",
    "df_limited = df.groupby('syndrome').head(300)\n",
    "\n",
    "# 保存到新的CSV文件\n",
    "df_limited.to_csv('TCM_syndrome_differentiation.csv', index=False)\n",
    "\n",
    "print(f\"筛选完成，共保存{len(df_limited)}条记录到output.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
